from interpreter.lexer import tokenize_source
from interpreter.lexer.token_types import TokenType


EXPECTED_TOKEN_TYPES = [
    TokenType.FLOAT_LITERAL,
    TokenType.SEMICOLON,
    TokenType.SPACE,
    TokenType.FLOAT_LITERAL,
    TokenType.SPACE,
    TokenType.INT_LITERAL,
    TokenType.SEMICOLON,
    TokenType.LEFT_BRACE,
    TokenType.RIGHT_BRACE,
    TokenType.PIPE,
    TokenType.COMMA,
    TokenType.INT_LITERAL,
    TokenType.SPACE,
    TokenType.INT_LITERAL,
    TokenType.IDENTIFIER,
    TokenType.SEMICOLON,
    TokenType.SPACE,
    TokenType.IDENTIFIER,
    TokenType.SPACE,
    TokenType.LINE_COMMENT,
    TokenType.STR_LITERAL,
    TokenType.PLUS_OP,
    TokenType.MULT_OP,
    TokenType.MINUS_OP,
    TokenType.DIV_OP,
    TokenType.LEFT_BRACKET,
    TokenType.RIGHT_BRACKET,
    TokenType.LEFT_PAR,
    TokenType.RIGHT_PAR,
    TokenType.SPACE,
    TokenType.RET_TYPE_MARK,
    TokenType.SPACE,
    TokenType.LOOP_KWD,
    TokenType.SPACE,
    TokenType.IF_KWD,
    TokenType.SPACE,
    TokenType.IFELSE_KWD,
    TokenType.ELSE_KWD,
    TokenType.SPACE,
    TokenType.IDENTIFIER,
    TokenType.SPACE,
    TokenType.EXCL_MARK,
    TokenType.QUEST_MARK,
    TokenType.SPACE,
    TokenType.RANGE_MARK,
    TokenType.FLOAT_LITERAL,
    TokenType.SEMICOLON,
    TokenType.LINE_COMMENT,
    TokenType.STOP_KWD,
    TokenType.SPACE,
    TokenType.NEXT_KWD,
    TokenType.SPACE,
    TokenType.EXCL_MARK,
    TokenType.SPACE,
    TokenType.FLOAT_LITERAL,
    TokenType.SPACE,
    TokenType.FLOAT_LITERAL,
    TokenType.GTE_OP,
    TokenType.LTE_OP,
    TokenType.LT_OP,
    TokenType.RET_TYPE_MARK,
    TokenType.IDENTIFIER,
    TokenType.EXCL_MARK,
    TokenType.AND_OP,
    TokenType.GT_OP,
    TokenType.OR_OP,
    TokenType.RANGE_MARK,
    TokenType.NOT_OP,
    TokenType.EOF,
]


def test_lexer_main():
    tokens = tokenize_source("tests/test_lexer.txt")

    assert len(tokens) == len(EXPECTED_TOKEN_TYPES)

    for i in range(len(tokens)):
        assert tokens[i].type == EXPECTED_TOKEN_TYPES[i]